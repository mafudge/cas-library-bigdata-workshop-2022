{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca8b196-d83c-43ea-a2d1-839eedb26bc2",
   "metadata": {},
   "source": [
    "# SQL Over Anything with Spark\n",
    "\n",
    "- Examples From Video Lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a9c15d-ab23-40ac-b0fb-69cfe1b32bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8117ffa1-f89e-4bf2-a0f3-5185020257b0;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.1.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.1.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 153ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.1.2 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8117ffa1-f89e-4bf2-a0f3-5185020257b0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "22/12/08 18:12:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/08 18:12:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/08 18:12:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/12/08 18:12:18 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/12/08 18:12:18 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/12/08 18:12:18 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "bucket = \"e-drill\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.1.2,org.apache.spark:spark-avro_2.12:3.1.2\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a08f444-c473-43ce-acae-5b1f952e2a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+---------------+\n",
      "|               Email|Gender|State|Months Customer|\n",
      "+--------------------+------+-----+---------------+\n",
      "|  afresco@dayrep.com|     M|   NY|              1|\n",
      "| cling@superrito.com|     F|   NY|              6|\n",
      "|etasomthin@superr...|     M|   NY|             28|\n",
      "|   jpoole@dayrep.com|     F|   NY|             12|\n",
      "| ojouglad@einrot.com|     M|   NY|             36|\n",
      "| rovlight@dayrep.com|     M|   NY|             42|\n",
      "| sladd@superrito.com|     M|   NY|             10|\n",
      "|titupp@superrito.com|     F|   NY|             42|\n",
      "| tpani@superrito.com|     M|   NY|              1|\n",
      "+--------------------+------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = spark.read.csv(\"file:///home/jovyan/datasets/customers/customers.csv\", \n",
    "                   inferSchema=True, header=True)\n",
    "c.toPandas()\n",
    "c.createOrReplaceTempView(\"customers\") # now its an SQL table in Spark!\n",
    "query = '''\n",
    "SELECT  Email, Gender, State, `Months Customer`\n",
    "FROM customers \n",
    "    WHERE State = 'NY'\n",
    "'''\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2f384-c5e6-4707-bdb2-59997b44c0c7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Put data in the right places!!!\n",
    "- Run these cells to ensure you have the data for the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a089d1-8e78-45b0-9669-8fbe1b431fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87a21da-98ef-4692-bd01-a620a834955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "\n",
    "# Make the minio bucket\n",
    "client = Minio(\"minio:9000\",\"minio\",\"SU2orange!\", secure=False)\n",
    "not client.bucket_exists(bucket) and client.make_bucket(bucket)\n",
    "\n",
    "# open the example(s)\n",
    "gp = spark.read.json(\"file:///home/jovyan/datasets/json-samples/google-places.json\")\n",
    "c = spark.read.csv(\"file:///home/jovyan/datasets/customers/customers.csv\", inferSchema=True, header=True)\n",
    "s = spark.read.csv(\"file:///home/jovyan/datasets/customers/surveys.csv\",inferSchema=True, header=True)\n",
    "g = spark.read.csv(\"file:///home/jovyan/datasets/grades/*.tsv\",inferSchema=False, header=False, sep=\"\\t\")\n",
    "k = spark.read.csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\",inferSchema=True, header=True)\n",
    "\n",
    "# Put the examples in minio\n",
    "gp.write.mode(\"Overwrite\").json(f\"s3a://{bucket}/google-places.json\")\n",
    "c.write.mode(\"Overwrite\").csv(f\"s3a://{bucket}/customers.csv\",header=True)\n",
    "s.write.mode(\"Overwrite\").csv(f\"s3a://{bucket}/surveys.csv\",header=True)\n",
    "g.write.mode(\"Overwrite\").csv(f\"s3a://{bucket}/grades.csv\",header=False, sep=\",\")\n",
    "g.write.mode(\"Overwrite\").parquet(f\"s3a://{bucket}/grades.parquet\")\n",
    "k.write.mode(\"Overwrite\").csv(f\"s3a://{bucket}/stocks.csv\",header=True,sep=\",\")\n",
    "\n",
    "#put the examples in HDFS\n",
    "gp.write.mode(\"Overwrite\").json(f\"hdfs://namenode/user/root/{bucket}/google-places.json\")\n",
    "c.write.mode(\"Overwrite\").csv(f\"hdfs://namenode/user/root/{bucket}/customers.csv\",header=True)\n",
    "s.write.mode(\"Overwrite\").csv(f\"hdfs://namenode/user/root/{bucket}/surveys.csv\",header=True)\n",
    "g.write.mode(\"Overwrite\").csv(f\"hdfs://namenode/user/root/{bucket}/grades.csv\",header=False, sep=\",\")\n",
    "g.write.mode(\"Overwrite\").parquet(f\"hdfs://namenode/user/root/{bucket}/grades.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd939c76-8290-4938-b211-ce76cc48c88f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fab600-f499-4b91-9b5e-24b94419f8e6",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "- Spark Supports ANSI Standard SQL\n",
    "- Once you register a DataFrame as a Temp View, you can query it like an SQL table.\n",
    "- The output is a DataFrame, that you can maniuplate further with spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "613c328a-d5cf-46d9-94b0-34e385a1d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Last IP Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Total Orders: integer (nullable = true)\n",
      " |-- Total Purchased: integer (nullable = true)\n",
      " |-- Months Customer: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Twitter Username: string (nullable = true)\n",
      " |-- Marital Status: string (nullable = true)\n",
      " |-- Household Income: string (nullable = true)\n",
      " |-- Own Home: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Favorite Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.createOrReplaceTempView(\"customers\")\n",
    "s.createOrReplaceTempView(\"surveys\")\n",
    "c.printSchema()\n",
    "s.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe978dc-bbee-40fd-ba63-e4ad906cb44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+---------------+--------+--------------------+\n",
      "|               Email|Gender|State|Months Customer|Own Home|    Household Income|\n",
      "+--------------------+------+-----+---------------+--------+--------------------+\n",
      "|etasomthin@superr...|     M|   NY|             28|      No|               39000|\n",
      "|   jpoole@dayrep.com|     F|   NY|             12|     Yes|Prefer not to Answer|\n",
      "| ojouglad@einrot.com|     M|   NY|             36|      No|               65000|\n",
      "| rovlight@dayrep.com|     M|   NY|             42|      No|               28000|\n",
      "| sladd@superrito.com|     M|   NY|             10|     Yes|               52000|\n",
      "+--------------------+------+-----+---------------+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Months Customer: integer (nullable = true)\n",
      " |-- Own Home: string (nullable = true)\n",
      " |-- Household Income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select c.Email, c.Gender, c.State, c.`Months Customer`, s.`Own Home`, s.`Household Income`\n",
    "from customers c left join surveys s on \n",
    "        c.Email = s.Email\n",
    "    where c.State = 'NY'\n",
    "    and c.`Months Customer` > 5\n",
    "    and s.`Own Home` is not null\n",
    "'''\n",
    "df = spark.sql(query)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936ed7ac-0e43-45c2-9ef3-744bc3523519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [Email#524, Gender#525, State#528, Months Customer#531, Own Home#97, Household Income#96]\n",
      "+- *(2) BroadcastHashJoin [Email#524], [Email#93], Inner, BuildRight, false\n",
      "   :- *(2) Filter ((((isnotnull(State#528) AND isnotnull(Months Customer#531)) AND (State#528 = NY)) AND (Months Customer#531 > 5)) AND isnotnull(Email#524))\n",
      "   :  +- FileScan csv [Email#524,Gender#525,State#528,Months Customer#531] Batched: false, DataFilters: [isnotnull(State#528), isnotnull(Months Customer#531), (State#528 = NY), (Months Customer#531 > 5..., Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/datasets/customers/customers.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State), IsNotNull(Months Customer), EqualTo(State,NY), GreaterThan(Months Customer,5),..., ReadSchema: struct<Email:string,Gender:string,State:string,Months Customer:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id=#365]\n",
      "      +- *(1) Filter (isnotnull(Own Home#97) AND isnotnull(Email#93))\n",
      "         +- FileScan csv [Email#93,Household Income#96,Own Home#97] Batched: false, DataFilters: [isnotnull(Own Home#97), isnotnull(Email#93)], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/datasets/customers/surveys.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Own Home), IsNotNull(Email)], ReadSchema: struct<Email:string,Household Income:string,Own Home:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6abe0a-c8e0-4819-90df-ee0e58bdaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.createOrReplaceTempView(\"customers\")\n",
    "s.createOrReplaceTempView(\"surveys\")\n",
    "c.printSchema()\n",
    "s.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a01321-9fe3-4eb6-bb52-0a1cbe956428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+-----+---------------+--------+------+\n",
      "|            Email|Gender|State|Months Customer|Own Home|Income|\n",
      "+-----------------+------+-----+---------------+--------+------+\n",
      "|jpoole@dayrep.com|     F|   NY|             12|     Yes|  null|\n",
      "+-----------------+------+-----+---------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = '''\n",
    "select c.Email, c.Gender, c.State, c.`Months Customer`, s.`Own Home`, \n",
    "    case s.`Household Income` \n",
    "        when 'Prefer not to Answer' then null \n",
    "        else cast(s.`Household Income` as DOUBLE) \n",
    "    end as Income\n",
    "from customers c left join surveys s on \n",
    "        c.Email = s.Email\n",
    "    where c.State = 'NY'\n",
    "    and c.`Months Customer` > 5\n",
    "    and s.`Own Home` is not null\n",
    "'''\n",
    "df = spark.sql(query)\n",
    "df.filter(\"Gender = 'F'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe23d1d-579b-4777-897c-a676177fe92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.explain()\n",
    "df.createOrReplaceTempView(\"v_ny_customer_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b9037-d240-4ee8-9c85-a332b4ba12af",
   "metadata": {},
   "source": [
    "## Spark SQL works with Nested Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d8f0b-07bb-41c1-b2c5-29f98be9a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acaa0bf4-f3a9-4882-bb24-769ebd528513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------------------+-----------------+\n",
      "|                name|              lat|               lng|             type|\n",
      "+--------------------+-----------------+------------------+-----------------+\n",
      "|            Syracuse|       43.0481221|-76.14742439999999|         locality|\n",
      "|            Syracuse|       43.0481221|-76.14742439999999|        political|\n",
      "|Crowne Plaza Syra...|       43.0476078|       -76.1417642|          lodging|\n",
      "|Crowne Plaza Syra...|       43.0476078|       -76.1417642|point_of_interest|\n",
      "|Crowne Plaza Syra...|       43.0476078|       -76.1417642|    establishment|\n",
      "|  The Parkview Hotel|       43.0476157|        -76.140986|          lodging|\n",
      "|  The Parkview Hotel|       43.0476157|        -76.140986|point_of_interest|\n",
      "|  The Parkview Hotel|       43.0476157|        -76.140986|    establishment|\n",
      "|Jefferson Clinton...|       43.0472894|-76.15385049999999|          lodging|\n",
      "|Jefferson Clinton...|       43.0472894|-76.15385049999999|point_of_interest|\n",
      "|Jefferson Clinton...|       43.0472894|-76.15385049999999|    establishment|\n",
      "|Courtyard by Marr...|       43.0488846|       -76.1561175|          lodging|\n",
      "|Courtyard by Marr...|       43.0488846|       -76.1561175|point_of_interest|\n",
      "|Courtyard by Marr...|       43.0488846|       -76.1561175|    establishment|\n",
      "|Quality Inn & Sui...|43.05264399999999|-76.14681999999999|          lodging|\n",
      "|Quality Inn & Sui...|43.05264399999999|-76.14681999999999|point_of_interest|\n",
      "|Quality Inn & Sui...|43.05264399999999|-76.14681999999999|    establishment|\n",
      "| Syracuse University|       43.0391534|       -76.1351158|       university|\n",
      "| Syracuse University|       43.0391534|       -76.1351158|point_of_interest|\n",
      "| Syracuse University|       43.0391534|       -76.1351158|    establishment|\n",
      "+--------------------+-----------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gp.createOrReplaceTempView(\"googleplaces\")\n",
    "query = '''\n",
    "select name, geometry.location.lat, geometry.location.lng, explode(types) as type from googleplaces\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e496ad-18ed-4047-a23b-b712e052a1ac",
   "metadata": {},
   "source": [
    "## Registering a UDF for use in Spark SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac863ca8-7340-41f7-bb6c-4bdd3fe910fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import * \n",
    "\n",
    "@udf(returnType=StringType()) \n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "spark.udf.register(\"upperCase\", upperCase)\n",
    "\n",
    "spark.sql(\"select Email, upperCase(Email), Gender from v_ny_customer_analysis\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f894bc-72c3-4cbe-9715-d6b29540eae1",
   "metadata": {},
   "source": [
    "## Listing views in Your Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143905e8-fc4b-4818-93e3-1bdac7c4ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show views\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a79dd9-36eb-45a4-b3b4-77f7ca4744fe",
   "metadata": {},
   "source": [
    "# Big Data To Small Data\n",
    "\n",
    "Using the `.toPandas()` function will materilaize the spark DataFrame as a Python Pandas dataframe. This allows us to use small-data features available in Python as our code is no longer executing on the cluster.\n",
    "\n",
    "## Graphing Output\n",
    "\n",
    "- A Spark DataFrame can be converted to a Python Pandas dataframe with the `toPandas()` function.\n",
    "- Once its in Pandas you can plot with standard plotting libraries.\n",
    "- https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690766ed-2e89-4ae9-8696-0a2785668435",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.toPandas().plot.bar(x=\"Email\", y=\"Total Purchased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72141e5c-33b5-4dee-86ab-0302cc564bd8",
   "metadata": {},
   "source": [
    "## IPywidgets\n",
    "\n",
    "- IPy Widgets allows us to create interactive notebooks\n",
    "- https://ipywidgets.readthedocs.io/en/latest/\n",
    "- This is a great example of going from \"Big Data\" to \"Small Data\"\n",
    "- As a rule, we don't want to produce small data (Pandas DataFrame) until we have mapped or reduced results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a286643-3bf4-4044-ae51-081b8f2a3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, interact_manual\n",
    "c = spark.read.csv(\"file:///home/jovyan/datasets/customers/customers.csv\", inferSchema=True, header=True)\n",
    "\n",
    "display(HTML(\"<h1>Customers By State</h1>\"))\n",
    "states = c.select(\"State\").distinct().toPandas()[\"State\"].values\n",
    "states.sort()\n",
    "@interact(state=states)\n",
    "def main(state :str):\n",
    "    rows = c.where( c.State == state).toPandas()\n",
    "    display(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12f3ec-7d83-4df9-9b72-c9f145f11fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "\n",
    "display(HTML(\"<h1>Customers By Total Purchased</h1>\"))\n",
    "rangewidget = widgets.IntRangeSlider(\n",
    "    value=[10, 1000],\n",
    "    min=0,\n",
    "    max=5000,\n",
    "    step=10,\n",
    "    description='Total Purchased:',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "\n",
    "c.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "@interact_manual(purchased=rangewidget)\n",
    "def main(purchased: int):\n",
    "    query = f'select * from customers where `Total Purchased` between {purchased[0]} and {purchased[1]}'\n",
    "    display(spark.sql(query).toPandas())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da17f1-409e-4f18-a8e2-afff59489dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0602e-ba1a-46c2-9bff-c191758f2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.orderBy(\"Favorite Department\", ascending = False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f39819-5172-472a-a68c-e5b08ed0a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.orderBy(s[\"Favorite Department\"],ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d6d33-ef39-4aef-8067-a6445e9dbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.filter( \"`Favorite Department` LIKE 'Prefer%' \" ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e0491-989b-44c2-93d3-69e93a0e69ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "s.filter(col(\"Favorite Department\").startswith(\"Prefer\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a0483-cf89-481e-ac53-58d5ddd72a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.createOrReplaceTempView(\"surveys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003604b6-065d-4701-929b-d78c607102e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b516b59-13bc-4c16-8418-8324498c471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z =spark.sql(\"select * from surveys where `Favorite Department` like 'Prefer%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471bc94-8737-4894-aa1b-6522336edbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = s.filter(col(\"Favorite Department\").startswith(\"Prefer\") )\n",
    "z =spark.sql(\"select * from surveys where `Favorite Department` like 'Prefer%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8fbb2-b246-4e10-86c5-3fb512a5ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64774c58-a554-4211-9330-fca16fe90ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb62da3-b8b3-4c07-b0b9-5ed7648590e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "SELECT *\n",
    "    FROM  surveys\n",
    "    WHERE `Favorite Department` like 'Prefer%'\n",
    "\n",
    "'''\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683dbd8-6b9f-47b8-abd5-af510dc999f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark\n",
    "spark.read.csv(\"s3a://....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9c374-3e89-4030-a832-513765686f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"csv\").load(\"s3a:///...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bd86d-da06-41e2-81b0-0d7e50f79f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.write.format(\"foo.bvar.avro\").mode(\"errorifexists\")\\\n",
    "    .option(\"sep\",\"-\").option(\"header\",True).save(\"file:///foo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f74f9d-8710-42d1-97dd-6d094f5332e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.select(\"geometry.location.lat\",gp.geometry.location.lng.alias(\"lng\")  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d38b5-22c5-4f78-9f21-bc60eed94abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.select(col(\"geometry.location.lat\").alias(\"g.l.lat\"),gp.geometry.location.lng.alias(\"g.l.lng\")  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783380a-8ffb-4629-a676-e4bedb087b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.select(\"geometry.location.lat\",gp.geometry.location.lng ).toDF(\"g.l.lat\",\"g.l.lng\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbc921-10ed-4a70-8fe2-012012c7dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766fff6-b16b-446e-8400-f4143b4da0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/mafudge/datasets/master/crime/nys-crime-1960-2012.csv\")\n",
    "df = spark.read.option(\"inferSchema\",True).option(\"header\", True)\\\n",
    "    .csv(SparkFiles.get(\"nys-crime-1960-2012.csv\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc2b3f-d78f-438f-b742-6329a6ed10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkFiles.get(\"nys-crime-1960-2012.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc42f9f-5ce9-4fc9-add3-d0ac0be71ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "\n",
    "@interact_manual(name=\"\", age=(1,99), retired=False, hobbies=[\"Biking\",\"Skiing\"])\n",
    "def onclick(name,age,retired,hobbies):\n",
    "    display(HTML(f\"<p>{name}, {age} {retired} {hobbies}\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d34af9-10e1-4b90-aa79-12c2112b5c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
