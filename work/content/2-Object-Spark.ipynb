{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "980b0a7e-32a3-4eb5-ba57-22a15400f334",
   "metadata": {},
   "source": [
    "# Object Storage and PySpark Programming\n",
    "\n",
    "- Examples From Video Lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cca4c16-c9d9-4c97-ba2c-4ca858fd063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "bucket = \"d-object-spark\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.1.2,org.apache.spark:spark-avro_2.12:3.1.2\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac9b2d3-4d45-4751-9f55-64cc6e399c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context :  <SparkContext master=local appName=jupyter-pyspark>\n",
      "Spark Version :  3.1.2\n",
      "Spark appName : jupyter-pyspark\n",
      "Hadoop version:  3.2.0\n",
      "Spark Confiuration:\n",
      "\tspark.master = local\n",
      "\thive.metastore.uris = thrift://hive-metastore:9083\n",
      "\tspark.submit.pyFiles = /home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.2.jar,/home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,/home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\tspark.app.name = jupyter-pyspark\n",
      "\tspark.hadoop.fs.s3a.path.style.access = True\n",
      "\tspark.serializer.objectStreamReset = 100\n",
      "\tspark.jars.packages = org.apache.hadoop:hadoop-aws:3.1.2,org.apache.spark:spark-avro_2.12:3.1.2\n",
      "\tspark.submit.deployMode = client\n",
      "\tspark.driver.port = 34137\n",
      "\tspark.hadoop.fs.s3a.fast.upload = True\n",
      "\tspark.app.initial.jar.urls = spark://jupyter:34137/jars/org.apache.hadoop_hadoop-aws-3.1.2.jar,spark://jupyter:34137/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,spark://jupyter:34137/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,spark://jupyter:34137/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\tspark.hadoop.fs.s3a.access.key = minio\n",
      "\tspark.driver.host = jupyter\n",
      "\tspark.jars = file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.2.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\tspark.hadoop.fs.s3a.secret.key = SU2orange!\n",
      "\tspark.executor.id = driver\n",
      "\tspark.app.startTime = 1644855780490\n",
      "\tspark.app.id = local-1644855781089\n",
      "\tspark.files = file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.2.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\tspark.app.initial.file.urls = file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.2.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar\n",
      "\tspark.driver.extraJavaOptions = -Dio.netty.tryReflectionSetAccessible=true\n",
      "\tspark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "\tspark.sql.catalogImplementation = hive\n",
      "\tspark.rdd.compress = True\n",
      "\tspark.hadoop.fs.s3a.endpoint = http://minio:9000\n",
      "\tspark.executor.extraJavaOptions = -Dio.netty.tryReflectionSetAccessible=true\n",
      "\tspark.sql.warehouse.dir = file:/home/jovyan/work/content/spark-warehouse/\n",
      "\tspark.ui.showConsoleProgress = true\n",
      "\tspark.repl.local.jars = file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.2.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n"
     ]
    }
   ],
   "source": [
    "# Print context\n",
    "print('Spark Context : ', spark.sparkContext)\n",
    "print('Spark Version : ', spark.sparkContext.version)\n",
    "print('Spark appName :', spark.sparkContext.appName)\n",
    "print('Hadoop version: ', spark.sparkContext._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion())\n",
    "print('Spark Confiuration:')\n",
    "for conf in spark.sparkContext._conf.getAll():\n",
    "    print(f\"\\t{conf[0]} = {conf[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea47639-14dd-412d-847b-058269deea61",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Put data in the right places!!!\n",
    "- Run these cells to ensure you have the data for the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be171b5-a42b-4464-b30b-a5b67f3fd95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minio\n",
      "  Downloading minio-7.1.5-py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 2.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /opt/conda/lib/python3.9/site-packages (from minio) (1.26.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from minio) (2021.5.30)\n",
      "Installing collected packages: minio\n",
      "Successfully installed minio-7.1.5\n"
     ]
    }
   ],
   "source": [
    "! pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e9d556-5cf5-4e68-b624-c1717a3f4b97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105/1986915933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make the minio bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minio:9000\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"minio\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SU2orange!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mnot\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# open the example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket' is not defined"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "\n",
    "# Make the minio bucket\n",
    "client = Minio(\"minio:9000\",\"minio\",\"SU2orange!\", secure=False)\n",
    "not client.bucket_exists(bucket) and client.make_bucket(bucket)\n",
    "\n",
    "# open the example \n",
    "df = spark.read.csv(\"/home/jovyan/datasets/stocks/stocks.csv\", inferSchema=true, header=True)\n",
    "\n",
    "# Put the example in minio\n",
    "df.write.mode(\"Overwrite\").csv(f\"s3a://{bucket}/stocks.csv\",header=True)\n",
    "\n",
    "#put the example in HDFS\n",
    "df.write.mode(\"Overwrite\").csv(f\"hdfs://namenode/user/root/{bucket}/stocks.csv\",header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e77e52-f221-4cf2-b306-4e6e537b8cf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Minio Client\n",
    "\n",
    "This section outlines commands from the minio client\n",
    "\n",
    "These commands are run from the terminal in your jupyter notebook setup\n",
    "\n",
    "### Minio Alias Setup\n",
    "\n",
    "```\n",
    "Installing the client\n",
    "\n",
    "$ wget https://dl.min.io/client/mc/release/linux-amd64/mc && chmod +x mc && sudo mv -f  mc /usr/local/bin\n",
    "\n",
    "# view aliases\n",
    "\n",
    "mc alias list\n",
    "\n",
    "# create alias to our server, which we will call \"ms\"\n",
    "\n",
    "mc alias set ms http://minio:9000 minio SU2orange!\n",
    "\n",
    "# to delete an alias its\n",
    "\n",
    "ms alias rm ms\n",
    "\n",
    "```\n",
    "\n",
    "### Minio File and bucket commands\n",
    "\n",
    "These are similar to the `hadoop fs` commands. \n",
    "\n",
    "```\n",
    "#make bucket testing \n",
    "mc mb play/testing\n",
    "\n",
    "# list buckets on the play alias\n",
    "mc ls play\n",
    "\n",
    "# copy files to the play/testing bucket\n",
    "\n",
    "mc cp /datasets/customers/* play/testing\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18e09e-103c-4554-9feb-02140d4f844e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab31cb43-5d49-4f8d-84fa-b5b76c0b9045",
   "metadata": {},
   "source": [
    "## Reading Data into the Spark Dataframe: Paths\n",
    "\n",
    "Spark can read (and write) data from a variety of locations, just by including the proper path to the file.\n",
    "\n",
    "- `file://` read a file off the local file system. Not ideal for a clustered environment. Use `SparkFiles`.\n",
    "- `s3a://` read from our object storage configration\n",
    "- `hdfs://` head from hadoop's HDFS using the client\n",
    "- `webhdfs://` head from hadoop's HDFS using the web client\n",
    "- `https://` read over the web - must use `SparkFiles`. See Next Section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dee5b48-1b0d-4f16-8ac1-1e8ebff16d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file://\n",
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|price,symbol|\n",
      "| 126.82,AAPL|\n",
      "|3098.12,AMZN|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "s3a://\n",
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|price,symbol|\n",
      "| 126.82,AAPL|\n",
      "|3098.12,AMZN|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "hdfs://\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://namenode/user/root/d-object-spark/stocks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_116/2123357695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"hdfs://namenode/user/root/{bucket}/stocks/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"webhdfs://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     def csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None,\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://namenode/user/root/d-object-spark/stocks"
     ]
    }
   ],
   "source": [
    "print(\"file://\") # Not Ideal!\n",
    "spark.read.text(\"file:///home/jovyan/datasets/stocks/stocks.csv\").show(3)\n",
    "\n",
    "print(\"s3a://\")\n",
    "spark.read.text(f\"s3a://{bucket}/stocks.csv\").show(3)\n",
    "\n",
    "print(\"hdfs://\")\n",
    "spark.read.text(f\"hdfs://namenode/user/root/{bucket}/stocks/\").show(3)\n",
    "\n",
    "print(\"webhdfs://\")\n",
    "spark.read.text(f\"webhdfs://namenode:50070/user/root/{bucket}/stocks/\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e998b-0229-4768-b1c7-eea6447fb70d",
   "metadata": {},
   "source": [
    "## Reading Data : `SparkFiles`\n",
    "\n",
    "Let's not forget Spark is a distributed computing environment. Reading a local file, or file off the web into our cluster doesn't help spark take advantage of its distributed nature. So to do that we need to use `SparkFiles` which registers the file with the `sparkContext` of the `sparkSession`. This, in essence makes the cluster aware of the file.\n",
    "\n",
    "`spark.sparkContext.addFile(url)` will  download the file at `url` and add it to the tmp location on the worker nodes in the cluster.\n",
    "\n",
    "When you need the file, use `SparkFiles.get(filename)` to retrieve its path.\n",
    "\n",
    "NOTES: \n",
    "\n",
    "- You add a file by path, but access the file by name. \n",
    "- You cannot add the same file name more than once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a172cb-dd6d-4c90-83da-81ff8473a4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary Location:  /tmp/spark-08af238d-31a5-4822-9179-b3e718641d77/userFiles-29a1326d-89b9-431f-9502-499f9bfbf500/stocks.csv\n",
      "https://\n",
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|price,symbol|\n",
      "| 126.82,AAPL|\n",
      "|3098.12,AMZN|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/mafudge/datasets/master/stocks/stocks.csv\")\n",
    "file_on_spark = SparkFiles.get(\"stocks.csv\")\n",
    "\n",
    "\n",
    "print(\"Temporary Location: \", SparkFiles.get(\"stocks.csv\"))\n",
    "\n",
    "print(\"https://\")\n",
    "spark.read.text(SparkFiles.get(\"stocks.csv\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc094b-cc0f-4dfb-a178-7aa7f8a5c105",
   "metadata": {},
   "source": [
    "## Reading Data : Wildcards\n",
    "\n",
    "You don't have to read a single file. Instead you can read an entire folder of files, or a wildcard match of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf38abed-8b10-4bbf-a072-9df3a46753fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read just fall\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2016\tFall\tIST346\t3\tA|\n",
      "|2016\tFall\tCHE111\t...|\n",
      "|2016\tFall\tPSY120\t...|\n",
      "|2016\tFall\tIST256\t3\tA|\n",
      "|2016\tFall\tENG121\t...|\n",
      "|2015\tFall\tIST101\t1\tA|\n",
      "|2015\tFall\tIST195\t3\tA|\n",
      "|2015\tFall\tIST233\t...|\n",
      "|2015\tFall\tSOC101\t...|\n",
      "|2015\tFall\tMAT221\t3\tC|\n",
      "+--------------------+\n",
      "\n",
      "read all files\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2016\tFall\tIST346\t3\tA|\n",
      "|2016\tFall\tCHE111\t...|\n",
      "|2016\tFall\tPSY120\t...|\n",
      "|2016\tFall\tIST256\t3\tA|\n",
      "|2016\tFall\tENG121\t...|\n",
      "|2015\tFall\tIST101\t1\tA|\n",
      "|2015\tFall\tIST195\t3\tA|\n",
      "|2015\tFall\tIST233\t...|\n",
      "|2015\tFall\tSOC101\t...|\n",
      "|2015\tFall\tMAT221\t3\tC|\n",
      "|2016\tSpring\tGEO11...|\n",
      "|2016\tSpring\tMAT22...|\n",
      "|2016\tSpring\tSOC12...|\n",
      "|2016\tSpring\tBIO24...|\n",
      "|2017\tSpring\tIST46...|\n",
      "|2017\tSpring\tMAT41...|\n",
      "|2017\tSpring\tSOC42...|\n",
      "|2017\tSpring\tENV20...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"read just fall\")\n",
    "spark.read.text(\"file:///home/jovyan/datasets/grades/fall*.tsv\").show()\n",
    "\n",
    "# read all of them\n",
    "print(\"read all files\")\n",
    "spark.read.text(\"file:///home/jovyan/datasets/grades\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32dadd-d08a-4b5c-8edd-a275226a6747",
   "metadata": {},
   "source": [
    "## Reading Data: File Formats\n",
    "\n",
    "Spark can read data in a variety of formats. Each format has configurable options.\n",
    "\n",
    "- `csv` delimited (comma, tab, etc) file\n",
    "- `text` generic text file, one row per line\n",
    "- `json` JSON format \n",
    "- `parquet` Parquet format (common big-data format with schema included)\n",
    "- `orc` Another common big-data format with schema.\n",
    "\n",
    "Each format has options to change behaviors of the file format. Use the `option()` method to set them.\n",
    "\n",
    "More Information: https://spark.apache.org/docs/latest/sql-data-sources.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3772252a-726e-4b27-9bdf-523b7c4e8b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "read Parquet file\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Read JSON file\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Read a pip-separated file\n",
      "+-------------------+--------------------+--------------------+--------+--------------------+\n",
      "|                _c0|                 _c1|                 _c2|     _c3|                 _c4|\n",
      "+-------------------+--------------------+--------------------+--------+--------------------+\n",
      "|2845428583999282239|1.4337661612984276E9|Mon Jun 08 08:22:...|rovlight|Why so horrible d...|\n",
      "|1658183905022391067|1.4298210344679017E9|Thu Apr 23 16:30:...|   sladd|Just placed an or...|\n",
      "| 973476786498736360|1.4421079524352274E9|Sat Sep 12 21:32:...| rdeboat|Worst purchase ev...|\n",
      "+-------------------+--------------------+--------------------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handle headers\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").show(3)\n",
    "\n",
    "# Infer schema from the columns\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").show(3)\n",
    "\n",
    "# readng a schema based file has less options\n",
    "print(\"read Parquet file\")\n",
    "spark.read \\\n",
    "    .parquet(\"file:///home/jovyan/datasets/stocks/stocks.parquet\").show(3)\n",
    "\n",
    "\n",
    "# JSON file format - there are many options for this file format\n",
    "print(\"Read JSON file\")\n",
    "spark.read.option(\"multiline\",True).json(\"/home/jovyan/datasets/json-samples/stocks.json\").show(3)\n",
    "\n",
    "# This is not comma-delimited\n",
    "print(\"Read a pip-separated file\")\n",
    "spark.read \\\n",
    "    .option(\"sep\",\"|\") \\\n",
    "    .option(\"header\",False) \\\n",
    "    .option(\"inferSchema\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/tweets/tweets.psv\").show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21038d68-ff56-4cad-ba8c-78a7aac98bd9",
   "metadata": {},
   "source": [
    "## Caching DataFrames\n",
    "\n",
    "The `cache()` function will persist the `DataFrame` to temp storage on the spark cluster. This can be in-memory, on disk, or both depending on the cluster size and data set size.\n",
    "\n",
    "This is specially useful when the data source is external to the spark cluster (a remote database, for example) and it will be retrieved and transformed multiple times.\n",
    "\n",
    "`cache()` forces lazy evaluation so any transformation prior to caching are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea9e3130-5f15-4cc2-a28f-64cc655437e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"s3a://\")\n",
    "stocks = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(f\"s3a://{bucket}/stocks.csv\").cache()\n",
    "stocks.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cf2fb-60e5-468f-ac19-158ffc9ced25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DataFrame Schemas\n",
    "\n",
    "Every spark dataframe has a schema, or collection of typed columns. The schema is stored in a `StructType` and the columns are `StructFields` consisting of the field name and a specific `StructType`\n",
    "\n",
    "- When you `spark.read` data, from  the schema is always the most flexible type, `StringType`.\n",
    "- When you include the `inferSchema` option, and extra pass is made over the data to infer the `StructType` for each column.\n",
    "- For formats that include a schema, like `parquet` or `orc` the schema in the file is loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4884abb-1993-401c-9307-99c3895d809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stocks: No Schema\")\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").printSchema()\n",
    "\n",
    "# Infer schema from the columns\n",
    "print(\"Stocks: Infer Schema\")\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").printSchema()\n",
    "\n",
    "\n",
    "# This is not comma-delimited\n",
    "print(\"Customers...\")\n",
    "customers = spark.read \\\n",
    "    .option(\"sep\",\",\") \\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/customers/customers.csv\")\n",
    "    \n",
    "customers.printSchema()\n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c79cda-1fe5-4407-930c-04f4d95b40bc",
   "metadata": {},
   "source": [
    "## DataFrame Schemas: Nested Schema\n",
    "\n",
    "Spark handles file formats with nested schemas, such as `json` very well. This means you can read from Document and Graph databases easily. \n",
    "\n",
    "- Embedded columns can be additional `StructType` columns or `ArrayType` for nested lists of values.\n",
    "- Later we will introduce strategies for dealing with nested schema like this one|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc2be1-77f4-416c-8399-aa91bc6f6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not comma-delimited\n",
    "print(\"Customers...\")\n",
    "places = spark.read \\\n",
    "    .json(\"file:///home/jovyan/datasets/json-samples/google-places.json\")\n",
    "    \n",
    "places.printSchema()\n",
    "places.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13762e3f-6fce-4f06-9d11-933c028827f8",
   "metadata": {},
   "source": [
    "## Column Transformations\n",
    "\n",
    " - `withColumnRenamed()` – rename a column\n",
    " - `toDF()` – rename all columns\n",
    " - `withColumn()` – overwrite an existing column, deriving new columns\n",
    " - `drop()` – remove a column\n",
    " - `select()` - column projections\n",
    "\n",
    "\n",
    "### Setting Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849b45fc-e5e9-42be-a408-8826c1144ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Columns Names... yuck\n",
      "+----+----+------+---+---+\n",
      "| _c0| _c1|   _c2|_c3|_c4|\n",
      "+----+----+------+---+---+\n",
      "|2016|Fall|IST346|  3|  A|\n",
      "|2016|Fall|CHE111|  4| A-|\n",
      "|2016|Fall|PSY120|  3| B+|\n",
      "|2016|Fall|IST256|  3|  A|\n",
      "|2016|Fall|ENG121|  3| B+|\n",
      "+----+----+------+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Rename first two columns\n",
      "+----+--------+------+---+---+\n",
      "|Year|Semester|   _c2|_c3|_c4|\n",
      "+----+--------+------+---+---+\n",
      "|2016|    Fall|IST346|  3|  A|\n",
      "|2016|    Fall|CHE111|  4| A-|\n",
      "|2016|    Fall|PSY120|  3| B+|\n",
      "|2016|    Fall|IST256|  3|  A|\n",
      "|2016|    Fall|ENG121|  3| B+|\n",
      "+----+--------+------+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Rename all the columns\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "+----+--------+------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/*.tsv\")\n",
    "\n",
    "print(\"Default Columns Names... yuck\")\n",
    "grades.show(5)\n",
    "\n",
    "print(\"Rename first two columns\")\n",
    "grades2 = grades.withColumnRenamed(\"_c0\",\"Year\").withColumnRenamed(\"_c1\",\"Semester\")\n",
    "grades2.show(5)\n",
    "\n",
    "print(\"Rename all the columns\")\n",
    "grades3 = grades.toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "grades3.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabb49d-a127-4949-a6be-0f2609fa27d3",
   "metadata": {},
   "source": [
    "### Derived Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97242b6-1026-4795-82a4-161c3b27161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving a column\n",
    "from pyspark.sql.functions import lit\n",
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "\n",
    "grades2 = grades.withColumn(\"Next Year\",grades[\"Year\"] + 1) \\\n",
    "    .withColumn(\"YearString\", grades['Year'].cast(\"String\") ) \\\n",
    "    .withColumn(\"NullCol\", lit(None) )\n",
    "grades2.printSchema()\n",
    "grades2.show()\n",
    "\n",
    "grades2 = grades2.drop(\"NullCol\").show()\n",
    "grades3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f2269-d592-4c0b-b4f6-07e734748e14",
   "metadata": {},
   "source": [
    "### Column Projections with `select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16339eb7-2039-428f-a8a7-73fa50a9a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Course|Grade|\n",
      "+------+-----+\n",
      "|IST346|    A|\n",
      "|CHE111|   A-|\n",
      "|PSY120|   B+|\n",
      "|IST256|    A|\n",
      "|ENG121|   B+|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----+\n",
      "|Course|Grade|\n",
      "+------+-----+\n",
      "|IST346|    A|\n",
      "|CHE111|   A-|\n",
      "|PSY120|   B+|\n",
      "|IST256|    A|\n",
      "|ENG121|   B+|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----+\n",
      "|Course|Grade|\n",
      "+------+-----+\n",
      "|IST346|    A|\n",
      "|CHE111|   A-|\n",
      "|PSY120|   B+|\n",
      "|IST256|    A|\n",
      "|ENG121|   B+|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "# string references\n",
    "grades.select(\"Course\", \"Grade\").show(5)\n",
    "\n",
    "# Object property references\n",
    "grades.select(grades.Course, grades.Grade).show(5)\n",
    "\n",
    "# Dataframe references\n",
    "grades.select(grades[\"Course\"], grades[\"Grade\"]).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ace579-dc7a-425b-9c14-1a754f02c5b5",
   "metadata": {},
   "source": [
    "## Row Transformations\n",
    "\n",
    "- `where()` or `filter()` apply a row based filter\n",
    "- `distinct()` remove duplicates\n",
    "- `sort()` or `orderBy()` sort by columns\n",
    "\n",
    "\n",
    "### Where / Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b5a33-1b31-42c4-8823-8c1cf38d902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "print(\"A grades\")\n",
    "# string references\n",
    "grades.filter(\"Grade = 'A' or Grade='A-'\").show()\n",
    "\n",
    "# Object property references\n",
    "grades.filter( (grades.Grade == \"A\") | (grades.Grade == \"A-\") ).show()\n",
    "\n",
    "# Dataframe references\n",
    "grades.filter( (grades[\"Grade\"] == \"A\") | (grades[\"Grade\"] == \"A-\") ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37d6c8-b5b4-4c44-a1b9-f84face566d2",
   "metadata": {},
   "source": [
    "### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c30ca80-db47-4418-add8-2e240df61cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = grades.select(\"Year\",\"Semester\")\n",
    "print(\"Terms\")\n",
    "terms.show()\n",
    "print(\"Distinct Terms\")\n",
    "dterms = terms.distinct()\n",
    "dterms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b6b12-b0b6-4696-831a-a842374aacf4",
   "metadata": {},
   "source": [
    "### Sort / orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2506588-2972-4dd7-8f6e-f30c629a954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "print(\"Sorting\")\n",
    "# string references\n",
    "grades.sort(\"Year\",\"Course\").show()\n",
    "\n",
    "# Object property references\n",
    "grades.sort(grades.Year, grades.Course.desc() ).show()\n",
    "\n",
    "# Dataframe references\n",
    "grades.sort( grades[\"Year\"], grades[\"Course\"].desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7a6c4-82e8-4cba-9c3d-428ed10394e4",
   "metadata": {},
   "source": [
    "## Aggregate Transformations\n",
    "\n",
    "- `groupBy()`  - perform a column grouping,similar to SQL group by,  returns a `GroupedData`\n",
    "- `agg()` - allows the application of an aggregate function to the `GroupedData`, returns a `DataFrame`\n",
    "- `alias()` - used to assign a name to a derived column\n",
    "- Aggregate Functions `count(), avg(), max(), min(), sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e7174-fa58-4c38-8d81-152ffcabe1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum,avg,max,min,count\n",
    "\n",
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "totalcredits = grades.groupBy().agg( sum(\"Credits\").alias(\"TotalCredits\"), count(\"*\").alias(\"CourseCount\") )\n",
    "totalcredits.show() \n",
    "\n",
    "termcredits = grades.groupBy(\"Year\", \"Semester\").agg( \\\n",
    "    count(\"*\").alias(\"CourseCount\"), sum(\"Credits\").alias(\"TotalCredits\") \\\n",
    "    ).sort(\"Year\",col(\"Semester\").desc())\n",
    "termcredits.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb453c9-637a-4e1a-b69b-c9d49eafef41",
   "metadata": {},
   "source": [
    "## Merge Transformations\n",
    "\n",
    "- `join()` -Merge data frame by column matching SQL join. Requires a join type string:\n",
    "    - \"inner\"  - SQL-like inner join\n",
    "    - \"full\" - SQL-like full outer join\n",
    "    - \"left\" - SQL-like left join\n",
    "    - \"right\" - SQL-Like right join\n",
    "    - \"cross\" - Cartesan Product \n",
    "- `union()` - merge two data frames by row, duplicates included, use `distinct()` to remove them.\n",
    "\n",
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a601eabd-3bf8-4419-8065-a2ca936118c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105/3733346326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradepoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file://home/jovyan/datasets/courses/grade-points.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrades\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradepoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrades\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrade\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgradepoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrades\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrades\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCourse\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCourse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "gradepoints = spark.read.option(\"inferSchema\",True).csv(\"file://home/jovyan/datasets/courses/grade-points.csv\")\n",
    "\n",
    "grades.join(gradepoints, grades.Grade == gradepoints.Grade, \"inner\").show()\n",
    "\n",
    "grades.join(courses, grades.Course == courses.Course, \"full\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776c3f0-6ade-470c-a3b7-62dda46cce97",
   "metadata": {},
   "source": [
    "### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae02694-1807-4c25-8621-b7aa3f0a548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "fallgrades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/fall*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "springgrades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/spring*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "fallgrades.show()\n",
    "springgrades.show()\n",
    "\n",
    "fallgrades.union(springgrades).show()\n",
    "\n",
    "print(\"Double the courses!\")\n",
    "grades.union(grades).groupBy().count().show()\n",
    "\n",
    "print(\"Filter out the duplicates\")\n",
    "grades.union(grades).distinct().groupby().count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbe3de-138d-4879-8b4b-b621b2cb72f5",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDF's)\n",
    "\n",
    "- User-defined functions allow us to write custom transformations. The process:\n",
    "\n",
    "1. Create python function, decorated for spark with `@func.udf(returnType=?)`, \n",
    "2. Apply function in `select()` or `withColumn()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2cd81a-6c33-47bd-a4cc-46822be0dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def term(year, semester):\n",
    "    return f\"{year}-{semester}\"\n",
    "\n",
    "\n",
    "@func.udf(returnType=BooleanType())\n",
    "def inMajor(course):\n",
    "    return course.startswith(\"IST\")\n",
    "\n",
    "\n",
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "grades.withColumn(\"Term\", term( grades.Year, grades.Semester) ).show()\n",
    "\n",
    "grades.select(\"Course\", inMajor(grades.Course).alias(\"InMajor\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07975d9-082e-4783-8593-209b757d9708",
   "metadata": {},
   "source": [
    "## Nested Column Transformations\n",
    "\n",
    "- Sometimes the schema is nested with additional `StructType` or `ArrayType` fields.\n",
    "- For nested `StructType` you can use the object property accessor to get to the nested columns.\n",
    "- For nested `ArrayType` you can use the `explode()` function to flatten the nested data. when you explode an array, the parent values will repeat for each value in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36671c8-b035-4a7d-bc0c-24aa2434f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "places = spark.read.json(\"file:///home/jovyan/datasets/json-samples/google-places.json\", multiLine=True)\n",
    "places.printSchema()\n",
    "places.show(5)\n",
    "\n",
    "print(\"Two places\")\n",
    "places.select('name','geometry.location.lat',places.geometry.location.lng, places['types']).show(2)\n",
    "\n",
    "print(\"Same two places, one row per type\")\n",
    "places.select('name','geometry.location.lat',places.geometry.location.lng, explode(places.types).alias(\"type\") ).show(5)\n",
    "\n",
    "print(\"Let's the the photo attributions\")\n",
    "places.select('name', explode( places.photos ).alias(\"col\") ) \\\n",
    "    .select(\"name\", explode(\"col.html_attributions\").alias(\"attributions\") ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a7ec6-e881-4f70-b88b-bb31160e2e4e",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "The `explain()` function will demonstrate the execution plan of the spark transformations. \n",
    "This is useful for understanding how the DAG processes the transformations. \n",
    "It should be noted that they are not processed in the order as written but instead processed  as optimized by spark.\n",
    "\n",
    "Notice in this example the last transformation is to filter the Year to 2016. In the Physical plan, this is one of the first transoformations. (You read the transformation graph from bottom to top).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85d052-16f6-454b-ac34-c0ba95614a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "termcredits = grades.groupBy(\"Year\", \"Semester\").agg( \\\n",
    "    count(\"*\").alias(\"CourseCount\"), sum(\"Credits\").alias(\"TotalCredits\") \\\n",
    "    ).sort(\"Year\",col(\"Semester\").desc())\n",
    "final = termcredits.filter(\"Year=2016\")\n",
    "final.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7d34881-5ef4-4935-af45-a26a7eb56556",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = grades.filter(\"year = 2016\")\\\n",
    "    .filter(grades.Semester == \"Fall\")\\\n",
    "    .sort(\"Course\") \\\n",
    "    .select(\"Course\", grades.Credits, grades[\"Grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94c110de-f775-4c90-9858-a53ef166b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Course#489 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Course#489 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#467]\n",
      "   +- *(1) Project [_c2#479 AS Course#489, _c3#480 AS Credits#490, _c4#481 AS Grade#491]\n",
      "      +- *(1) Filter (((isnotnull(_c0#477) AND isnotnull(_c1#478)) AND (_c0#477 = 2016)) AND (_c1#478 = Fall))\n",
      "         +- FileScan csv [_c0#477,_c1#478,_c2#479,_c3#480,_c4#481] Batched: false, DataFilters: [isnotnull(_c0#477), isnotnull(_c1#478), (_c0#477 = 2016), (_c1#478 = Fall)], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/datasets/grades/fall2015.tsv, file:/home/jovyan/datasets/grad..., PartitionFilters: [], PushedFilters: [IsNotNull(_c0), IsNotNull(_c1), EqualTo(_c0,2016), EqualTo(_c1,Fall)], ReadSchema: struct<_c0:int,_c1:string,_c2:string,_c3:int,_c4:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ce023ac-da49-479f-beb7-faab9b28e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = grades.sort(\"Course\") \\\n",
    "    .filter(grades.Semester == \"Fall\")\\\n",
    "    .select(\"Course\", grades.Credits, grades[\"Grade\"])\\\n",
    "    .filter(\"year = 2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3fc590f-934c-46f1-87d4-429855a377f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Course#489 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Course#489 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#488]\n",
      "   +- *(1) Project [_c2#479 AS Course#489, _c3#480 AS Credits#490, _c4#481 AS Grade#491]\n",
      "      +- *(1) Filter (((isnotnull(_c1#478) AND isnotnull(_c0#477)) AND (_c1#478 = Fall)) AND (_c0#477 = 2016))\n",
      "         +- FileScan csv [_c0#477,_c1#478,_c2#479,_c3#480,_c4#481] Batched: false, DataFilters: [isnotnull(_c1#478), isnotnull(_c0#477), (_c1#478 = Fall), (_c0#477 = 2016)], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/datasets/grades/fall2015.tsv, file:/home/jovyan/datasets/grad..., PartitionFilters: [], PushedFilters: [IsNotNull(_c1), IsNotNull(_c0), EqualTo(_c1,Fall), EqualTo(_c0,2016)], ReadSchema: struct<_c0:int,_c1:string,_c2:string,_c3:int,_c4:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d38a668e-c62d-4c0d-9aa2-e26314f92908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[price: string, symbol: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e36fe964-927f-4292-b82b-60e591d35a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "|  45.11|  TWTR|\n",
      "|   78.0|   NET|\n",
      "| 126.82|  AAPL|\n",
      "| 128.39|   IBM|\n",
      "| 212.55|  MSFT|\n",
      "| 251.11|    FB|\n",
      "|  497.0|  NFLX|\n",
      "|  823.8|  TSLA|\n",
      "|1725.05|  GOOG|\n",
      "|3098.12|  AMZN|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "#df.withColumn(\"price\", df.price.cast(DoubleType())).printSchema().sort(df[\"price\"]).toPandas()\n",
    "\n",
    "df.sort(df.price.cast(\"Float\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbd5eb-9f7f-4843-9900-380e7ff30b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
