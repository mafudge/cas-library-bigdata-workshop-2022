{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6cac071-f24a-48a9-b639-0501372a313d",
   "metadata": {},
   "source": [
    "# Document Database Model\n",
    "\n",
    "- Examples From Video Lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d793ae9-eaed-4ff6-a116-fea3dcdb9421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4ea8c498-c724-445f-8e87-c795b6883f3c;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (93ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (58ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (149ms)\n",
      ":: resolution report :: resolve 911ms :: artifacts dl 334ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4ea8c498-c724-445f-8e87-c795b6883f3c\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (2728kB/7ms)\n",
      "22/12/08 18:41:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/08 18:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/08 18:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/12/08 18:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/12/08 18:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/12/08 18:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/12/08 18:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    }
   ],
   "source": [
    "# Spark init\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "mongo_uri = \"mongodb://admin:mongopw@mongo:27017/admin?authSource=admin\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "      .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "      .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "      .config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea940f-06cb-4972-879b-341283300864",
   "metadata": {},
   "source": [
    "## Loading Sample Data\n",
    "\n",
    "- Run this code to load some sample data into MongoDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27450bc0-b2cd-4186-b18b-861effba8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Loading Sample Data\n",
    "from pyspark.sql.functions import col\n",
    "s = spark.read.option(\"multiline\",\"true\").json(\"file:///home/jovyan/datasets/json-samples/stocks.json\")\n",
    "spark.read.option(\"multiline\",\"true\").json(\"file:///home/jovyan/datasets/json-samples/europe.json\")\\\n",
    "    .write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\"demo\").option(\"collection\",\"europe\").save()\n",
    "spark.read.option(\"multiline\",\"true\").json(\"file:///home/jovyan/datasets/json-samples/fudgemart-products.json\")\\\n",
    "    .withColumn(\"_id\", col(\"product_id\")).write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\"demo\").option(\"collection\",\"products\").save()\n",
    "spark.read.option(\"multiline\",\"true\").json(\"file:///home/jovyan/datasets/json-samples/US-Senators.json\")\\\n",
    "    .write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\"demo\").option(\"collection\",\"senators\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07928d9-49b9-4e6d-97b4-2f87795aef3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90c68a40-cb75-49c9-af6c-f56bdbbbb53b",
   "metadata": {},
   "source": [
    "## MongoDB Clients and Applications:\n",
    "\n",
    "- The **Mongo Db Shell** is the offical client where you can type in the MQL (Mongo Query Language)  \n",
    "  `PS> docker-compose exec mongo mongosh -u admin -p mongopw --authenticationDatabase=admin`\n",
    "- **Mongo Express** is a web-based database administration application, http://localhost:8881\n",
    "- There is a **Sample Python Application** here: http://localhost:5081\n",
    "- To Connect MongoDb to Tools like Tableau or PowerBI, use an ODBC driver like this one here:  \n",
    "  https://github.com/mongodb/mongo-bi-connector-odbc-driver/releases/\n",
    "  \n",
    "  \n",
    "## Mongo Shell Queries\n",
    "\n",
    "\n",
    "### MQL: Create and Read\n",
    "\n",
    "```\n",
    "# current Database \n",
    "db\n",
    "\n",
    "# Show all databases\n",
    "show databases\n",
    "\n",
    "\n",
    "# Use a database, does not have to exist - mongo don't care!\n",
    "use demo\n",
    "\n",
    "#show collections \n",
    "show collections\n",
    "\n",
    "# insert some data – how did it make the collection – mongo don’t care!\n",
    "db.cars.insertOne({ \"make\": \"Chevy\", \"model\" : \"Cruze\" })\n",
    "\n",
    "# insert a couple....\n",
    "db.cars.insertMany( [ { \"make\": \"Chevy\", \"model\" : \"Traverse\" }, { \"make\": \"Chevy\", \"model\" : \"Trax\", \"mpg\" : 36} ] )\n",
    "\n",
    "# show all documents\n",
    "db.cars.find()\n",
    "\n",
    "# No a car? Mongo don't care!\n",
    "db.cars.insertOne( { \"name\" : \"Mike Fudge\", \"age\" : 50 } )\n",
    "db.cars.find()\n",
    "\n",
    "\n",
    "# Insert same thing twice … mongo don’t care!\n",
    "db.cars.insertOne({ \"make\": \"Honda\", \"model\" : \"Civic\"})\n",
    "\n",
    "# oops no schema but I forgot mpg...\n",
    "db.cars.insertOne({ \"make\": \"Honda\", \"model\" : \"Civic\", \"mpg\" : 40 })\n",
    "\n",
    "## Simple query By Value\n",
    "\n",
    "db.cars.find( { \"make\" : \"Chevy\" })\n",
    "\n",
    "```\n",
    "\n",
    "### MQL:Understanding_id\n",
    "\n",
    "```\n",
    "# Insert Multiple Times \n",
    "\n",
    "db.cars.insertOne( { \"make\" : \"Honda\", \"model\" : \"CRV\"})\n",
    "db.cars.insertOne( { \"make\" : \"Honda\", \"model\" : \"CRV\"})\n",
    "\n",
    "# added once\n",
    "db.cars.insertOne( { \"make\" : \"Honda\", \"model\" : \"CRV\", \"_id\" : 1 })\n",
    "\n",
    "# cannot be added again! At least there is key integrity.\n",
    "db.cars.insertOne( { \"make\" : \"Honda\", \"model\" : \"CRV\", \"_id\" : 1 })\n",
    "```\n",
    "\n",
    "### MQL: Updating and Deleting Data\n",
    "\n",
    "\n",
    "```\n",
    "# delete the first one that matches \n",
    "db.cars.deleteOne( { \"model\" : \"CRV\" } )\n",
    "\n",
    "bb.cars.find()\n",
    "\n",
    "# delete all that match\n",
    "db.cars.deleteMany( { \"model\" : \"CRV\" } )\n",
    "\n",
    "# delete something that's not there\n",
    "db.cars.deleteMany( { \"model\" : \"Fabio\" })\n",
    "\n",
    "# delete by Object Id\n",
    "db.cars.find({ \"name\" : \"Mike Fudge\" })\n",
    "(record object ID)\n",
    "db.cars.deleteOne({\"_id\" : ObjectId(\"6203d8c48017a5f57d121bf6\")})\n",
    "\n",
    "# delete by an object ID, Non Surrogate\n",
    "db.cars.deletOne( { \"_id\" : 1 } )\n",
    "\n",
    "# replace – no partial updates\n",
    "db.cars.insertOne( { \"make\" : \"Honda\", \"model\" : \"CRV\", \"_id\" : 2 })\n",
    "\n",
    "db.cars.replaceOne({ \"_id\" : 2},  { \"mpg\" : 26 } )\n",
    "\n",
    "#where did it go?\n",
    "bb.cars.find()\n",
    "\n",
    "# full overwrite, so you must replace\n",
    "db.cars.replaceOne({ \"_id\" : 2},  { \"make\" : \"Honda\", \"model\" : \"CRV\", \"mpg\": 26, \"_id\" : 2 })\n",
    "\n",
    "\n",
    "# updates - add MPG to traverse\n",
    "\n",
    "db.cars.updateOne({model: 'Traverse'}, {$set : {mpg:18}})\n",
    "\n",
    "# update multiple values\n",
    "db.cars.updateOne({model: 'Traverse'}, {$set : {mpg:16, \"type\": \"SUV\" }})\n",
    "\n",
    "#update several documents\n",
    "\n",
    "db.cars.updateMany({}, { $set: { \"owner\": \"mafudge\"}} )\n",
    "\n",
    "```\n",
    "\n",
    "### Find Queries\n",
    "\n",
    "```\n",
    "# no filter,  just ask for 3 columns (notice we get nothing for license plate)\n",
    "\n",
    "db.cars({}, { make:1, model:2, \"license place\":3 }) \n",
    "\n",
    "# here's a complex filter: make is chevy and includes an mpg\n",
    "db.cars.find({ $and : [  {make : \"Chevy\"}, {mpg : { $exists: true } } ] })\n",
    "\n",
    "# let's combine that:\n",
    "db.cars.find({ $and : [  {make : \"Chevy\"}, {mpg : { $exists: true } } ] }, { make:1, model:2, mpg:3, \"license place\":6 })\n",
    "\n",
    "#and let's sort that\n",
    "db.cars.find({ $and : [  {make : \"Chevy\"}, {mpg : { $exists: true } } ] }, { make:1, model:2, mpg:3, \"license place\":6 }).sort({mpg:-1})\n",
    "\n",
    "```\n",
    "\n",
    "### Indexing\n",
    "\n",
    "\n",
    "```\n",
    "# querying by region!\n",
    "db.europe.find( {\"subregion\" : \"Eastern Europe\"}).explain(\"executionStats\")\n",
    "\n",
    "Seaches through all 53 countries…. Blah. (docsExamined)\n",
    "COLLSCAN is like a TABLE SCAN in SQL\n",
    "\n",
    "# Let’s add an index.\n",
    "db.europe.createIndex( {subregion:1})\n",
    "\n",
    "db.europe.find( {\"subregion\" : \"Eastern Europe\"}).explain(\"executionStats\")\n",
    "db.europe.find( {\"subregion\" : “Southern Europe\"}).explain(\"executionStats\")\n",
    "\n",
    "Now its doing an IXSCAN (index scan) and looking at the keysExamined!\n",
    "```\n",
    "\n",
    "## Drilling Mongo\n",
    "\n",
    "```\n",
    "\n",
    "Storage config is easy!\n",
    "\n",
    "{\n",
    "  \"type\": \"mongo\",\n",
    "  \"connection\": \"mongodb://admin:mongopw@mongo:27017/admin\",\n",
    "  \"enabled\": true\n",
    "}\n",
    "\n",
    "some drills:\n",
    "\n",
    "# avg population by subregion \n",
    "select subregion, avg(population) as avg_pop, count(*) as county_count \n",
    "    from mongo.demo.europe\n",
    "    group by subregion \n",
    "\n",
    "# russian timeszones\n",
    "select name, population, flatten(timezones) from mongo.demo.europe where name = 'Russia'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630647e-7a21-42f5-b04c-8fab775e9feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fbf4ca-a054-4061-81a4-e9ebaea24b66",
   "metadata": {},
   "source": [
    "## MongoDb In Spark\n",
    "\n",
    "- MongoDb has first-class support for Spark\n",
    "- When using filters with DataFrames API, the underlying Mongo Connector code constructs an aggregation pipeline to filter the data in MongoDB before sending it to Spark.\n",
    "- This ensures only the data needed is retrieved from MongoDb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb192be-6ad3-490b-8fe3-121d1a150a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data, Surrogate key ID\n",
    "s = spark.read.option(\"multiline\",\"true\").json(\"file:///home/jovyan/datasets/json-samples/stocks.json\")\n",
    "s.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\").option(\"database\",\"demo\")\\\n",
    "    .option(\"collection\",\"stocks\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6871d04-3094-4687-9bf4-c092859719b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data, assign an existing column as the \"_id\" before write.\n",
    "s.withColumn(\"_id\",s.symbol).write.format(\"mongo\")\\\n",
    "    .mode(\"overwrite\").option(\"database\",\"demo\").option(\"collection\",\"stocks2\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95048c4c-9ecd-41a9-b8ef-63460605874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| _id|  price|symbol|\n",
      "+----+-------+------+\n",
      "|AAPL| 126.82|  AAPL|\n",
      "|AMZN|3098.12|  AMZN|\n",
      "|  FB| 251.11|    FB|\n",
      "|GOOG|1725.05|  GOOG|\n",
      "| IBM| 128.39|   IBM|\n",
      "|MSFT| 212.55|  MSFT|\n",
      "| NET|   78.0|   NET|\n",
      "|NFLX|  497.0|  NFLX|\n",
      "|TSLA|  823.8|  TSLA|\n",
      "|TWTR|  45.11|  TWTR|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s2 = spark.read.format(\"mongo\").option(\"database\",\"demo\").option(\"collection\",\"stocks2\").load()\n",
    "s2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c2808d-554c-43e6-85d1-100e5a104b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|_id| price|symbol|\n",
      "+---+------+------+\n",
      "|IBM|128.39|   IBM|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s2.createOrReplaceTempView(\"stocks\")\n",
    "s3 = spark.sql(\"SELECT * FROM stocks WHERE symbol='IBM'\")\n",
    "s3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2efdba4d-3123-4bd5-837e-32b1330dee1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(_id#351) AND (_id#351 = NFLX))\n",
      "+- *(1) Scan MongoRelation(MongoRDD[65] at RDD at MongoRDD.scala:51,Some(StructType(StructField(_id,StringType,true), StructField(price,DoubleType,true), StructField(symbol,StringType,true)))) [_id#351,price#352,symbol#353] PushedFilters: [IsNotNull(_id), EqualTo(_id,NFLX)], ReadSchema: struct<_id:string,price:double,symbol:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nfstock = s2.filter(\"_id = 'NFLX'\")\n",
    "nfstock.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c983e24-4c99-42e6-80e3-c2cb93baebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [price#322, symbol#323, symbol#323 AS _id#411]\n",
      "+- *(1) Filter (isnotnull(symbol#323) AND (symbol#323 = NFLX))\n",
      "   +- FileScan json [price#322,symbol#323] Batched: false, DataFilters: [isnotnull(symbol#323), (symbol#323 = NFLX)], Format: JSON, Location: InMemoryFileIndex[file:/home/jovyan/datasets/json-samples/stocks.json], PartitionFilters: [], PushedFilters: [IsNotNull(symbol), EqualTo(symbol,NFLX)], ReadSchema: struct<price:double,symbol:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nfs = s.withColumn(\"_id\",s.symbol).filter(\"_id = 'NFLX'\")\n",
    "nfs.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ba0d4-ade7-416c-87f9-6e17d1965e68",
   "metadata": {},
   "source": [
    "## Understanding how the Mongo Spark Connector Builds the aggregation pipeline\n",
    "\n",
    "In this example we demonstrate how the MongoDb connect passes most of the DataFrame tranformation logic directly to MongoDb thereby reducing the amount of computational effort expected of the Spark cluster.\n",
    "\n",
    "- `localq` processes all the transformations on spark. This consumes more memory and CPU on the spark cluster\n",
    "- `mongoq` Applies a PushedFilters and ReadSchema to MongoDb, meaning only \"Northern Europe\" documents and only the \"alpha3Code\", \"name\",\"subregion\", \"population\", and \"borders\" columns are being sent from MongoDb to Spark. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ae5f19-b368-42ec-b056-61d56306b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_euro = spark.read.option(\"multiline\",\"true\").json(\"file:///home/jovyan/datasets/json-samples/europe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "316cfa5b-4d5e-4527-9548-9ed4e823dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_euro.write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\"fdoc\").option(\"collection\",\"europe\").save()\n",
    "mongo_euro = spark.read.format(\"mongo\").option(\"database\",\"fdoc\").option(\"collection\",\"europe\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c296a915-996e-4092-bbef-14ab4de1d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+----------+----------------+\n",
      "|alpha3Code|               name|      subregion|population|borderAlpha3Code|\n",
      "+----------+-------------------+---------------+----------+----------------+\n",
      "|       DNK|            Denmark|Northern Europe|   5678348|             DEU|\n",
      "|       EST|            Estonia|Northern Europe|   1313271|             LVA|\n",
      "|       EST|            Estonia|Northern Europe|   1313271|             RUS|\n",
      "|       FIN|            Finland|Northern Europe|   5485215|             NOR|\n",
      "|       FIN|            Finland|Northern Europe|   5485215|             SWE|\n",
      "|       FIN|            Finland|Northern Europe|   5485215|             RUS|\n",
      "|       IRL|Republic of Ireland|Northern Europe|   6378000|             GBR|\n",
      "+----------+-------------------+---------------+----------+----------------+\n",
      "only showing top 7 rows\n",
      "\n",
      "+----------+-------------------+---------------+----------+----------------+\n",
      "|alpha3Code|               name|      subregion|population|borderAlpha3Code|\n",
      "+----------+-------------------+---------------+----------+----------------+\n",
      "|       DNK|            Denmark|Northern Europe|   5678348|             DEU|\n",
      "|       EST|            Estonia|Northern Europe|   1313271|             LVA|\n",
      "|       EST|            Estonia|Northern Europe|   1313271|             RUS|\n",
      "|       FIN|            Finland|Northern Europe|   5485215|             NOR|\n",
      "|       FIN|            Finland|Northern Europe|   5485215|             SWE|\n",
      "|       FIN|            Finland|Northern Europe|   5485215|             RUS|\n",
      "|       IRL|Republic of Ireland|Northern Europe|   6378000|             GBR|\n",
      "+----------+-------------------+---------------+----------+----------------+\n",
      "only showing top 7 rows\n",
      "\n",
      "plan for local file\n",
      "== Physical Plan ==\n",
      "*(2) Project [alpha3Code#416, name#427, subregion#433, population#430L, borderAlpha3Code#595]\n",
      "+- Generate explode(borders#419), [alpha3Code#416, name#427, population#430L, subregion#433], false, [borderAlpha3Code#595]\n",
      "   +- *(1) Filter (((isnotnull(subregion#433) AND (subregion#433 = Northern Europe)) AND (size(borders#419, true) > 0)) AND isnotnull(borders#419))\n",
      "      +- FileScan json [alpha3Code#416,borders#419,name#427,population#430L,subregion#433] Batched: false, DataFilters: [isnotnull(subregion#433), (subregion#433 = Northern Europe), (size(borders#419, true) > 0), isno..., Format: JSON, Location: InMemoryFileIndex[file:/home/jovyan/datasets/json-samples/europe.json], PartitionFilters: [], PushedFilters: [IsNotNull(subregion), EqualTo(subregion,Northern Europe), IsNotNull(borders)], ReadSchema: struct<alpha3Code:string,borders:array<string>,name:string,population:bigint,subregion:string>\n",
      "\n",
      "\n",
      "plan for MongoDb\n",
      "== Physical Plan ==\n",
      "*(2) Project [alpha3Code#550, name#561, subregion#567, population#564L, borderAlpha3Code#602]\n",
      "+- Generate explode(borders#553), [alpha3Code#550, name#561, population#564L, subregion#567], false, [borderAlpha3Code#602]\n",
      "   +- *(1) Filter ((((size(borders#553, true) > 0) AND isnotnull(subregion#567)) AND (subregion#567 = Northern Europe)) AND isnotnull(borders#553))\n",
      "      +- *(1) Scan MongoRelation(MongoRDD[99] at RDD at MongoRDD.scala:51,Some(StructType(StructField(_id,StructType(StructField(oid,StringType,true)),true), StructField(alpha2Code,StringType,true), StructField(alpha3Code,StringType,true), StructField(altSpellings,ArrayType(StringType,true),true), StructField(area,DoubleType,true), StructField(borders,ArrayType(StringType,true),true), StructField(callingCodes,ArrayType(StringType,true),true), StructField(capital,StringType,true), StructField(currencies,ArrayType(StringType,true),true), StructField(demonym,StringType,true), StructField(gini,DoubleType,true), StructField(languages,ArrayType(StringType,true),true), StructField(latlng,ArrayType(DoubleType,true),true), StructField(name,StringType,true), StructField(nativeName,StringType,true), StructField(numericCode,StringType,true), StructField(population,LongType,true), StructField(region,StringType,true), StructField(relevance,StringType,true), StructField(subregion,StringType,true), StructField(timezones,ArrayType(StringType,true),true), StructField(topLevelDomain,ArrayType(StringType,true),true), StructField(translations,StructType(StructField(de,StringType,true), StructField(es,StringType,true), StructField(fr,StringType,true), StructField(it,StringType,true), StructField(ja,StringType,true)),true)))) [alpha3Code#550,borders#553,name#561,population#564L,subregion#567] PushedFilters: [IsNotNull(subregion), EqualTo(subregion,Northern Europe), IsNotNull(borders)], ReadSchema: struct<alpha3Code:string,borders:array<string>,name:string,population:bigint,subregion:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "#local_euro.printSchema()\n",
    "\n",
    "# Heres a DataFrame transformation. This could be in SQL too.\n",
    "localq = local_euro.select(\"alpha3Code\", \"name\",\"subregion\", \"population\", explode(col(\"borders\")).alias(\"borderAlpha3Code\")).filter(\"subregion = 'Northern Europe'\")\n",
    "mongoq = mongo_euro.select(\"alpha3Code\", \"name\",\"subregion\", \"population\", explode(col(\"borders\")).alias(\"borderAlpha3Code\")).filter(\"subregion = 'Northern Europe'\")\n",
    "\n",
    "localq.show(7)\n",
    "mongoq.show(7)\n",
    "\n",
    "print(\"plan for local file\")\n",
    "localq.explain()\n",
    "\n",
    "print(\"plan for MongoDb\")\n",
    "mongoq.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778c96c-771f-4918-80d8-7d8f85637423",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419991e-f726-4983-9106-4cb0321c3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_euro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c0d0e-3995-436d-aa86-dd6bd4f67f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
